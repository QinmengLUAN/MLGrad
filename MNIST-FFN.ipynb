{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88a8c91a99fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pylab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "!pip install pycm livelossplot\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a more or less drop-in replacement to numpy functionality but with added automatic differentiation. Here's some basic torch.Tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "#A scalar value\n",
    "a = torch.Tensor(1)\n",
    "print(a, a.item())\n",
    "\n",
    "b = torch.zeros_like(a)\n",
    "print(b, b.item())\n",
    "\n",
    "c = torch.from_numpy(np.array(range(42)))\n",
    "print(c)\n",
    "\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randn(1, 1, 28, 28)\n",
    "print(m.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m[0, 0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "    \n",
    "tensor_on_device = torch.ones(1).to(device)\n",
    "print(tensor_on_device.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\"./\", download=True, train=True)\n",
    "mnist_test = MNIST(\"./\", download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create splits of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.train_data, mnist_train.train_labels)\n",
    "indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_normalization(X):\n",
    "  X /= 255.\n",
    "  X -= 0.1307\n",
    "  X /= 0.3081\n",
    "  return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = apply_normalization(mnist_train.train_data[indices[0]].float()), mnist_train.train_labels[indices[0]]\n",
    "X_val, y_val = apply_normalization(mnist_train.train_data[indices[1]].float()), mnist_train.train_labels[indices[1]]\n",
    "X_test, y_test =  apply_normalization(mnist_test.test_data.float()), mnist_test.test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = TensorDataset(X_train, y_train.long())\n",
    "mnist_validate = TensorDataset(X_val, y_val.long())\n",
    "mnist_test = TensorDataset(X_test, y_test.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0])\n",
    "print(X_train.mean(), X_train.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SimpleNet as a nn.Module\n",
    "Create a simple feed-forward neural network with the following architecture:\n",
    "- Input Layer: 28*28 neurons\n",
    "- Hidden Layer: 25 neurons\n",
    "- Output Layer: 10 neurons\n",
    "- Activation: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(28*28, 25)\n",
    "        self.linear_2 = nn.Linear(25, 10)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.linear_1(x)\n",
    "        a1 = self.activation(z1)\n",
    "        z2 = self.linear_2(a1)\n",
    "        return z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "X_ = X_train[0].view(-1, 28*28).to(device)\n",
    "y_ = torch.zeros((1)).to(device).long()\n",
    "\n",
    "a2 = model(X_)\n",
    "loss = criterion(a2, y_)\n",
    "\n",
    "y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "\n",
    "print(F.log_softmax(a2, dim=1))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple train function\n",
    "The function should perform the following tasks:\n",
    "- Set the model into training mode\n",
    "- Iterate over all the mini-batches\n",
    "- Send the batches to the GPU / CPU\n",
    "- Zero all the gradients\n",
    "- Perform the forward-pass\n",
    "- Compute the loss\n",
    "- Perform the backward-pass\n",
    "- Keep a running measure of training loss and accuracy\n",
    "- Perform a step of gradient-descent\n",
    "- Once done with all batches, return average training loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, data_loader):\n",
    "    model.train()\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        a2 = model(X.view(-1, 28*28))\n",
    "        loss = criterion(a2, y)\n",
    "        loss.backward()\n",
    "        train_loss += loss*X.size(0)\n",
    "        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n",
    "        optimizer.step()  \n",
    "        \n",
    "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple validate function\n",
    "The function should perform the following tasks:\n",
    "- Set the model into evaluation mode\n",
    "- Iterate over all the mini-batches\n",
    "- Send the batches to the GPU / CPU\n",
    "- Perform the forward-pass\n",
    "- Compute the loss\n",
    "- Keep a running measure of validation loss and accuracy\n",
    "- Once done with all batches, return average validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    validation_loss, validation_accuracy = 0., 0.\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 28*28))\n",
    "            loss = criterion(a2, y)\n",
    "            validation_loss += loss*X.size(0)\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n",
    "            \n",
    "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()\n",
    "for epoch in range(30):\n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "    \n",
    "    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "    logs['val_' + 'log loss'] = validation_loss.item()\n",
    "    logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model may not be the best. Let's do some hyperparameter optimisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(momentum):\n",
    "  set_seed(seed)\n",
    "  model = SimpleNet().to(device)\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "  validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "  test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "  \n",
    "  liveloss = PlotLosses()\n",
    "  for epoch in range(30):\n",
    "      logs = {}\n",
    "      train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "      logs['' + 'log loss'] = train_loss.item()\n",
    "      logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "      validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "      logs['val_' + 'log loss'] = validation_loss.item()\n",
    "      logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "\n",
    "      liveloss.update(logs)\n",
    "      liveloss.draw()\n",
    "      \n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    ys, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 28*28))\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            ys.append(y.cpu().numpy())\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n",
    "\n",
    "y_pred, y_gt = evaluate(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(actual_vector=y_gt, predict_vector=y_pred) # Create CM From Data\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\"./\", download=True, train=True)\n",
    "\n",
    "X_train, y_train = apply_normalization(mnist_train.train_data.float()), mnist_train.train_labels\n",
    "mnist_train = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "set_seed(seed)\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=best_momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(n_epochs):\n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "\n",
    "test_loss, test_accuracy = validate(model, criterion, test_loader)    \n",
    "print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: \n",
    "- Tune the learning rate\n",
    "- Add one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
